# Interaction Data and Analysis Pipeline

Extraction scripts, methodology documentation, and sample data for the interaction analysis reported in the paper.

## Pipeline Overview

```
Claude Code JSONL files (~1,457 files)
        |
        v
extract_prompts.py          Classify human prompts (25 categories)
        |                   Extract token usage, tools, agent spawns
        v                   Output: prompts.csv, agent_prompts.csv, prompts_monthly.csv
reconstruct_sessions.py     Fill gap period from indirect artifacts
        |                   (git commits, todo files, shell snapshots)
        v                   Output: reconstructed records merged into prompts.csv
analyze_impact.py           Correlate sessions with git commits
                            Calculate lines changed per session
                            Output: impact rankings, daily summaries
```

## Scripts

| Script | Lines | Purpose |
|--------|-------|---------|
| `extract_prompts.py` | ~706 | Core extraction: parse JSONL, classify prompts into 25 categories, compute token costs, detect agent spawns |
| `reconstruct_sessions.py` | ~811 | Reconstruct lost gap period (Dec 15 -- Jan 15) from git commits, todo files, and shell snapshots |
| `analyze_impact.py` | ~300 | Correlate prompt timestamps with git commits to identify high-impact interactions |

### Running the Scripts

The scripts expect Claude Code conversation history in the standard location:

```bash
# Extract prompts (requires JSONL files at ~/.claude/projects/{project-path}/)
python extract_prompts.py --stats

# Reconstruct gap period (requires git repo access)
python reconstruct_sessions.py --preview --stats

# Analyze impact (requires prompts.csv from extraction + git repo)
python analyze_impact.py --top 20
```

**Note:** The raw JSONL conversation files are not included in this repository due to size (~1.5 GB) and privacy considerations. The scripts are provided for methodology transparency and reproducibility with your own Claude Code conversation data.

## Methodology Documentation

| Document | Description |
|----------|-------------|
| `data-collection-methodology.md` | Full methodology: data sources, quality tiers, reconstruction approach, agent counting |
| `data-extraction-methodology.md` | Extraction details: JSONL file structure, parsing approach, data quality notes |

## Sample Data

| File | Description |
|------|-------------|
| `samples/prompts_monthly.csv` | Monthly usage summary (session counts, prompt counts, token usage by month) |
| `samples/key_plans.csv` | Planning session records showing multi-agent orchestration patterns |

### CSV Schema (prompts.csv)

The main output file `prompts.csv` (not included; generated by `extract_prompts.py`) has these columns:

| Column | Type | Description |
|--------|------|-------------|
| `id` | int | Sequential prompt ID |
| `timestamp` | ISO 8601 | When the prompt was sent |
| `date` | YYYY-MM-DD | Date component |
| `time` | HH:MM:SS | Time component |
| `prompt` | string | Truncated prompt text (first 200 chars) |
| `word_count` | int | Number of words in the prompt |
| `char_count` | int | Number of characters |
| `category` | string | Primary category (25 categories) |
| `category_secondary` | string | Secondary category (if applicable) |
| `session_id` | string | Conversation session UUID |
| `model` | string | Model used for response |
| `response_tokens_in` | int | Input tokens consumed |
| `response_tokens_out` | int | Output tokens generated |
| `tools_used` | string | Comma-separated tool names |
| `tool_count` | int | Number of tool calls |
| `agents_spawned` | int | Number of sub-agents launched |
| `agent_types` | string | Comma-separated agent type names |
| `platform` | string | Operating system (macOS/Windows) |

### Prompt Categories (25)

The classification system uses keyword matching with priority ordering:

| Category | Keywords (sample) |
|----------|-------------------|
| networking | network, multiplayer, sync, lobby, host, client |
| combat | damage, health, projectile, ability, attack |
| rendering | render, sprite, animation, camera, texture |
| ui | menu, button, hud, interface, overlay |
| ecs | entity, component, system, archetype |
| dungeon | dungeon, procedural, bsp, room, generation |
| ai | ai, behavior, enemy, chase, pathfind |
| debugging | fix, bug, wrong, broken, crash, error |
| architecture | refactor, cleanup, organize, restructure |
| documentation | doc, readme, comment, explain |
| ... | (15 more categories in extract_prompts.py) |

## Case Study Evidence

The `case-study-evidence/` directory contains a README indexing the JSONL conversation transcripts referenced in Section 4.8 of the paper. The actual JSONL files (18 files, ~149 MB) are preserved separately and not included here.

## Key Statistics (from the paper)

| Metric | Value |
|--------|-------|
| Total human prompts | 2,801 |
| Median prompt length | 14 words |
| Prompts <= 20 words | 52% |
| Agent invocations | 1,197 |
| Total agent turns | 16,522 |
| Amplification ratio | 5.9x (turns per invocation) |
| Development cost | ~$693 across 283 sessions |
| Cost per session | ~$2.45 |
